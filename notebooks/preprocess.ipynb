{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001_sweep1', '001_sweep2', '001_sweep3', '001_sweep4', '001_sweep5', '002_sweep1', '002_sweep2', '002_sweep3', '002_sweep5', '002_sweep6', '003_sweep1', '003_sweep2', '003_sweep3', '003_sweep4', '004_sweep1', '004_sweep2', '004_sweep4', '004_sweep5', '004_sweep6', '004_sweep7', '004_sweep8', '005_sweep1', '005_sweep2', '005_sweep3', '005_sweep4', '005_sweep5', '005_sweep6', '006_sweep1', '006_sweep4', '006_sweep5', '006_sweep6', '006_sweep7', '007_sweep1', '007_sweep2', '007_sweep3', '007_sweep4', '007_sweep6', '007_sweep7', '007_sweep8', '008_sweep1', '008_sweep2', '008_sweep3', '008_sweep6', '008_sweep7', '008_sweep8', '009_sweep1', '009_sweep2', '009_sweep3', '009_sweep4', '009_sweep5', '009_sweep6', '009_sweep7', '010_sweep1', '010_sweep2', '010_sweep3', '010_sweep4', '010_sweep6', '011_sweep1 (1)', '011_sweep1', '011_sweep2 (1)', '011_sweep2', '011_sweep3 (1)', '011_sweep3', '011_sweep4 (1)', '011_sweep4', '011_sweep5 (1)', '011_sweep5', '011_sweep6 (1)', '011_sweep6', '012_sweep1', '012_sweep2', '012_sweep3', '012_sweep4', '012_sweep6', '012_sweep7', '013_sweep1', '013_sweep2', '013_sweep3', '013_sweep4', '013_sweep5', '013_sweep6', '013_sweep7', '014_sweep1', '014_sweep2', '014_sweep3', '014_sweep4', '014_sweep5', '014_sweep6', '014_sweep7', '014_sweep8', '015_sweep1', '015_sweep2', '015_sweep4', '015_sweep5', '016_sweep1', '016_sweep2', '016_sweep4', '016_sweep6', '016_sweep7']\n",
      "['001', '001', '001', '001', '001', '002', '002', '002', '002', '002', '003', '003', '003', '003', '004', '004', '004', '004', '004', '004', '004', '005', '005', '005', '005', '005', '005', '006', '006', '006', '006', '006', '007', '007', '007', '007', '007', '007', '007', '008', '008', '008', '008', '008', '008', '009', '009', '009', '009', '009', '009', '009', '010', '010', '010', '010', '010', '011', '011', '011', '011', '011', '011', '011', '011', '011', '011', '011', '011', '012', '012', '012', '012', '012', '012', '013', '013', '013', '013', '013', '013', '013', '014', '014', '014', '014', '014', '014', '014', '014', '015', '015', '015', '015', '016', '016', '016', '016', '016']\n",
      "['1', '2', '3', '4', '5', '1', '2', '3', '5', '6', '1', '2', '3', '4', '1', '2', '4', '5', '6', '7', '8', '1', '2', '3', '4', '5', '6', '1', '4', '5', '6', '7', '1', '2', '3', '4', '6', '7', '8', '1', '2', '3', '6', '7', '8', '1', '2', '3', '4', '5', '6', '7', '1', '2', '3', '4', '6', '1', '1', '2', '2', '3', '3', '4', '4', '5', '5', '6', '6', '1', '2', '3', '4', '6', '7', '1', '2', '3', '4', '5', '6', '7', '1', '2', '3', '4', '5', '6', '7', '8', '1', '2', '4', '5', '1', '2', '4', '6', '7']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import natsort\n",
    "general_path = '/Users/emilio/Library/CloudStorage/Box-Box/GitHub/lightning-template/'\n",
    "ob1_files = glob.glob(general_path+'./LabelDataObstetrics1/*.mat')\n",
    "ob1_files = natsort.natsorted(ob1_files)\n",
    "#split ob1_files.split('/')\n",
    "patient_t=[]\n",
    "sweep_t=[]\n",
    "for i in range(len(ob1_files)):\n",
    "    ob1_files[i]=ob1_files[i].split('/')[-1].split('.')[0]\n",
    "    patient=ob1_files[i].split('_')[0]\n",
    "    sweep=ob1_files[i].split('sweep')[1]\n",
    "    #if sweep[0] is not a number, use sweep[1]\n",
    "    sweep = sweep[1] if not sweep[0].isdigit() else sweep[0]\n",
    "    patient_t.append(patient)\n",
    "    sweep_t.append(sweep)\n",
    "print(ob1_files)\n",
    "print(patient_t)\n",
    "print(sweep_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['011_sweep5', '005_sweep4', '012_sweep4', '011_sweep1', '011_sweep3', '002_sweep5', '014_sweep3', '001_sweep5', '005_sweep2', '004_sweep1', '003_sweep2', '013_sweep1', '009_sweep3', '012_sweep1', '014_sweep2', '014_sweep5', '016_sweep4', '013_sweep3', '006_sweep1', '016_sweep6', '014_sweep6', '012_sweep6', '002_sweep6', '015_sweep1', '004_sweep6', '014_sweep1', '003_sweep1', '008_sweep1', '006_sweep7', '004_sweep8', '008_sweep3', '011_sweep4', '006_sweep6', '010_sweep1', '005_sweep1', '007_sweep7', '013_sweep5', '009_sweep4', '002_sweep3', '015_sweep4', '001_sweep3', '011_sweep2', '005_sweep3', '005_sweep6', '007_sweep4', '009_sweep5', '006_sweep4', '012_sweep7', '011_sweep6', '003_sweep4', '013_sweep6', '007_sweep8', '015_sweep2', '014_sweep8', '014_sweep7', '008_sweep2', '016_sweep7', '009_sweep6', '004_sweep4', '015_sweep5', '002_sweep1', '008_sweep6', '004_sweep2', '012_sweep3', '007_sweep2', '004_sweep5', '012_sweep2', '009_sweep7', '004_sweep7', '001_sweep1', '005_sweep5', '008_sweep7', '016_sweep2', '007_sweep6', '001_sweep4', '013_sweep2', '013_sweep4', '009_sweep2', '013_sweep7', '008_sweep8', '010_sweep4', '009_sweep1', '006_sweep5', '010_sweep2', '007_sweep1', '016_sweep1', '002_sweep2', '007_sweep3', '001_sweep2', '014_sweep4', '010_sweep6', '003_sweep3', '010_sweep3']\n",
      "99\n",
      "93\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "ob1_files = glob.glob(general_path+'./LabelDataObstetrics1/*.mat')\n",
    "ob1_files = natsort.natsorted(ob1_files)\n",
    "#split ob1_files.split('/')\n",
    "ob1_files=[i.split('/')[-1].split('.')[0] for i in ob1_files]\n",
    "npy_files = glob.glob(general_path+'./data/128x128x128/*.npy')\n",
    "npy_files = natsort.natsorted(npy_files)\n",
    "npy_files=list(set([i.split('/')[-1].split('.')[0].split('_gt')[0].split('_label')[0] for i in npy_files]))\n",
    "print(npy_files)\n",
    "print(len(ob1_files))\n",
    "print(len(npy_files))\n",
    "#find the difference between the two lists\n",
    "diff = list(set(npy_files) - set(ob1_files))\n",
    "print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emilio/Library/CloudStorage/Box-Box/GitHub/lightning-template/./Study/013/013_sweep7.mp4\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import skvideo.io as skv\n",
    "import torchio as tio\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.morphology as morph\n",
    "\n",
    "src_path = os.path.join(general_path, 'src')\n",
    "sys.path.append(src_path)\n",
    "\n",
    "#import overlay_video from src/utils/overlay_video.py assuming that the file is in the same directory\n",
    "from utils.overlay_video import generate_overlay_video\n",
    "\n",
    "ob1_files = glob.glob(general_path+'./LabelDataObstetrics1/*.mat')\n",
    "ob1_files = natsort.natsorted(ob1_files)\n",
    "\n",
    "for i in range(81,82):\n",
    "# for i in range(2,3):\n",
    "     mp4_file_path = general_path+'./Study/'+patient_t[i]+'/'+patient_t[i]+'_sweep'+sweep_t[i]+'.mp4'\n",
    "     print(mp4_file_path)\n",
    "     ob = sio.loadmat(ob1_files[i])['labels']\n",
    "     ob=np.moveaxis(ob, 2, 0)\n",
    "     gt_video = skv.vread(mp4_file_path)\n",
    "     diff = gt_video[1:,:,:,0]-gt_video[:-1,:,:,0]\n",
    "     #remove small objects using morphological operations\n",
    "     # diff = morph.remove_small_objects(diff.astype(bool), min_size=100)\n",
    "\n",
    "     diff_sum = np.sum(diff,axis=(1,2))\n",
    "     #normalize diff_sum\n",
    "     diff_sum = diff_sum/np.max(diff_sum)\n",
    "\n",
    "     # Assuming diff_sum is your normalized data array from the plot\n",
    "     # Example: diff_sum = np.array([...])  # Replace with your actual array\n",
    "     threshold = 0.2\n",
    "\n",
    "     # Detect indices where diff_sum crosses above the threshold\n",
    "     first_frame = np.where(diff_sum > threshold)[0][0]  # First upward crossing\n",
    "     last_frame = np.where(diff_sum > threshold)[0][-1]  # Last upward crossing (before drop)\n",
    "     # print(diff.shape)\n",
    "\n",
    "     # Obtener los límites del área de interés\n",
    "     row_limits, col_limits = np.where(morph.remove_small_objects(diff[first_frame+50].astype(bool), min_size=100) > 0)\n",
    "     y1, y2 = row_limits.min(), row_limits.max()\n",
    "     x1, x2 = col_limits.min(), col_limits.max()\n",
    "\n",
    "     # Recortar el área de interés\n",
    "     gt_video = gt_video[first_frame:last_frame, y1:y2, x1:x2, 0]\n",
    "     ob = ob[first_frame:last_frame, :,:]\n",
    "     #print ob frames where is not zero\n",
    "\n",
    "\n",
    "     # Check shapes between gt_video and ob for width and height, and align them by cropping or padding centered\n",
    "     if gt_video.shape[1] != ob.shape[1] or gt_video.shape[2] != ob.shape[2]:\n",
    "          # Calculate padding or cropping for height (dim 1)\n",
    "          pad_or_crop_h = (gt_video.shape[1] - ob.shape[1])\n",
    "          pad_h_before = max(0, pad_or_crop_h // 2)\n",
    "          pad_h_after = max(0, pad_or_crop_h - pad_h_before)\n",
    "          crop_h_before = max(0, -pad_or_crop_h // 2)\n",
    "          crop_h_after = max(0, -pad_or_crop_h - crop_h_before)\n",
    "\n",
    "          # Calculate padding or cropping for width (dim 2)\n",
    "          pad_or_crop_w = (gt_video.shape[2] - ob.shape[2])\n",
    "          pad_w_before = max(0, pad_or_crop_w // 2)\n",
    "          pad_w_after = max(0, pad_or_crop_w - pad_w_before)\n",
    "          crop_w_before = max(0, -pad_or_crop_w // 2)\n",
    "          crop_w_after = max(0, -pad_or_crop_w - crop_w_before)\n",
    "\n",
    "          # Apply padding if needed\n",
    "          if pad_h_before > 0 or pad_h_after > 0 or pad_w_before > 0 or pad_w_after > 0:\n",
    "               ob = np.pad(ob, \n",
    "                              ((0, 0), \n",
    "                              (pad_h_before, pad_h_after), \n",
    "                              (pad_w_before, pad_w_after)), \n",
    "                              'constant')\n",
    "\n",
    "          # Apply cropping if needed\n",
    "          if crop_h_before > 0 or crop_h_after > 0 or crop_w_before > 0 or crop_w_after > 0:\n",
    "               ob = ob[:, crop_h_before:ob.shape[1] - crop_h_after, crop_w_before:ob.shape[2] - crop_w_after]\n",
    "\n",
    "     # Now, `ob` will have the same shape as `gt_video`\n",
    "     # Create a TorchIO subject with paired data\n",
    "     subject = tio.Subject(\n",
    "     video_gt=tio.ScalarImage(tensor=np.expand_dims(gt_video,axis=0)),  # Add channel dimension\n",
    "     label=tio.LabelMap(tensor=np.expand_dims(ob,axis=0))  # Add channel dimension\n",
    "     )\n",
    "\n",
    "     #efine the resize transform\n",
    "     resize_transform = tio.transforms.Resize((128,128,128))  # New shape: [frames, width, height]\n",
    "\n",
    "     # Apply the resize transform to the subject\n",
    "     resized_subject = resize_transform(subject)\n",
    "\n",
    "     # Extract resized video GT and label from the subject\n",
    "     resized_video_gt = resized_subject.video_gt.tensor.squeeze(0).numpy()  # Remove channel dimension\n",
    "     resized_label = resized_subject.label.tensor.squeeze(0).numpy().astype(int)  # Ensure integers for label\n",
    "\n",
    "     num_classes = 6  # Número de clases\n",
    "     # Crear la versión one-hot encoded\n",
    "     one_hot = np.eye(num_classes)[resized_label]  # Esto genera el one-hot encoding\n",
    "     # Verificar el nuevo shape\n",
    "     # print(one_hot.shape) \n",
    "     #generate_overlay_video(resized_video_gt, one_hot, fps=30, output_video=general_path+'data/128x128x128'+patient_t[i]+'_sweep'+sweep_t[i]+'_overlay.avi',gt_w=True)\n",
    "     #save the label with ground truth in npz format\n",
    "     #np.savez_compressed(general_path+'data/'+patient_t[i]+'_sweep'+sweep_t[i]+'.npz', video=resized_video_gt, label=resized_label.astype(np.uint8))\n",
    "     #save with np.save as npy gt and label\n",
    "     np.save(general_path+'data/128x128x128/'+patient_t[i]+'_sweep'+sweep_t[i]+'_gt.npy', resized_video_gt)\n",
    "     np.save(general_path+'data/128x128x128/'+patient_t[i]+'_sweep'+sweep_t[i]+'_label.npy', resized_label.astype(np.uint8))\n",
    "\n",
    "     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# #for read mat file\n",
    "# import scipy.io as sio\n",
    "# import skvideo.io as skv\n",
    "# import glob\n",
    "# import torch\n",
    "# general_path = '/Users/emilio/Documents/GitHub/lightning-template/'\n",
    "# #compare mat files between LabelDataObstetrics and LabelDataObstetrics1\n",
    "# mps_device = torch.device(\"mps\")\n",
    "# mp4_file_path = general_path+'Study/008/008_sweep7.mp4'\n",
    "\n",
    "\n",
    "# ob_files = glob.glob(general_path+'/LabelDataObstetrics/*.mat')\n",
    "# ob1_files = glob.glob(general_path+'./LabelDataObstetrics1/*.mat')\n",
    "# mp4_path= glob.glob(general_path+'./Study/')\n",
    "\n",
    "\n",
    "# maddie_002_1=sio.loadmat(general_path+'LabelDataObstetrics/008_sweep7 copy 2.mat')['labels']\n",
    "# #move 3rd dimension to 1st dimension\n",
    "# maddie_002_1 = np.moveaxis(maddie_002_1,2,0)\n",
    "# marika_002_1=sio.loadmat(general_path+'LabelDataObstetrics1/008_sweep7.mat')['labels']\n",
    "# #move 3rd dimension to 1st dimension\n",
    "# marika_002_1 = np.moveaxis(marika_002_1,2,0)\n",
    "# gt_video = skv.vread(mp4_file_path)\n",
    "# print(maddie_002_1.shape)\n",
    "# print(marika_002_1.shape)\n",
    "# print(gt_video.shape)\n",
    "\n",
    "# #check i where masks are not empty in maddie_002_1\n",
    "# for i in range(maddie_002_1.shape[0]):\n",
    "#     if np.sum(maddie_002_1[i])>0:\n",
    "#         print(i)\n",
    "#         break\n",
    "# #check i where masks are not empty in marika_002_1\n",
    "# for i in range(marika_002_1.shape[0]):\n",
    "#     if np.sum(marika_002_1[i])>0:\n",
    "#         print(i)\n",
    "#         break\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
